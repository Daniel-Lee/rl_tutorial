{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confirmed-custom",
   "metadata": {},
   "source": [
    "# Soft Actor Critic (SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,datetime,gym,os,time,psutil,cv2,scipy.signal,pybullet_envs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Box, Discrete\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "from collections import deque,namedtuple\n",
    "%matplotlib inline\n",
    "gym.logger.set_level(40)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "print (\"gym version:[%s]\"%(gym.__version__))\n",
    "print (\"TF:[%s]\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-invitation",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "class SACBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a SAC agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size=5000):\n",
    "        self.obs1_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.acts_buf = np.zeros(combined_shape(size, adim), dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size  # buffer has to have room so you can store\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs1=self.obs1_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     acts=self.acts_buf[idxs],\n",
    "                     rews=self.rews_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: v for k, v in batch.items()}\n",
    "\n",
    "    def get(self):\n",
    "        names = ['obs1_buf','obs2_buf','acts_buf','rews_buf','done_buf',\n",
    "                 'ptr','size','max_size']\n",
    "        vals =[self.obs1_buf,self.obs2_buf,self.acts_buf,self.rews_buf,self.done_buf,\n",
    "               self.ptr,self.size,self.max_size]\n",
    "        return names,vals\n",
    "\n",
    "    def restore(self,a):\n",
    "        self.obs1_buf = a[0]\n",
    "        self.obs2_buf = a[1]\n",
    "        self.acts_buf = a[2]\n",
    "        self.rews_buf = a[3]\n",
    "        self.done_buf = a[4]\n",
    "        self.ptr = a[5]\n",
    "        self.size = a[6]\n",
    "        self.max_size = a[7]\n",
    "\n",
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(\n",
    "        plt.gcf(),animate,frames=len(frames),interval=10)\n",
    "    display(display_animation(anim))\n",
    "    \n",
    "print (\"Done.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-premium",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(odim=24, hdims=[256, 256], actv='relu', output_actv='relu'):\n",
    "    ki = tf.keras.initializers.truncated_normal(stddev=0.1)\n",
    "    layers = tf.keras.Sequential()\n",
    "    layers.add(tf.keras.layers.InputLayer(input_shape=(odim,)))\n",
    "    for hdim in hdims[:-1]:\n",
    "        layers.add(tf.keras.layers.Dense(hdim,activation=actv,kernel_initializer=ki))\n",
    "    layers.add(tf.keras.layers.Dense(hdims[-1],activation=output_actv,kernel_initializer=ki))\n",
    "    return layers\n",
    "\n",
    "def gaussian_loglik(x,mu,log_std):\n",
    "    EPS = 1e-8\n",
    "    pre_sum = -0.5*(( (x-mu)/(tf.exp(log_std)+EPS) )**2 + 2*log_std + np.log(2*np.pi))\n",
    "    return tf.reduce_sum(pre_sum, axis=1)\n",
    "\n",
    "class GaussianPolicy(tf.keras.Model):\n",
    "    def __init__(self,odim,adim,hdims=[256,256],actv='relu'):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.odim    = odim\n",
    "        self.adim    = adim\n",
    "        self.hdims   = hdims\n",
    "        self.actv    = actv\n",
    "        # Define network\n",
    "        self.net     = mlp(self.odim,self.hdims,self.actv,output_actv=actv)\n",
    "        self.mu      = tf.keras.layers.Dense(self.adim,activation=None)\n",
    "        self.log_std = tf.keras.layers.Dense(self.adim,activation=None)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, o, get_logprob=True):\n",
    "        net_ouput = self.net(o)\n",
    "        mu = self.mu(net_ouput)\n",
    "        log_std = self.log_std(net_ouput)\n",
    "\n",
    "        LOG_STD_MIN, LOG_STD_MAX = -10.0, +2.0\n",
    "        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX) #log_std\n",
    "        std = tf.exp(log_std) \n",
    "        dist = tfp.distributions.Normal(mu, std)\n",
    "        pi = dist.sample()   \n",
    "\n",
    "        if get_logprob:\n",
    "            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "            # NOTE: The correction formula is a little bit magic. To get an understanding\n",
    "            # of where it comes from, check out the original SAC paper (arXiv 1801.01290)\n",
    "            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "            # Try deriving it yourself as a (very difficult) exercise. :)\n",
    "            logp_pi = gaussian_loglik(x=pi, mu=mu, log_std=log_std)\n",
    "            logp_pi -= tf.reduce_sum(2*(np.log(2) - pi - tf.nn.softplus(-2*pi)), axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "        mu, pi = tf.tanh(mu), tf.tanh(pi) # squach action \n",
    "        return mu, pi, logp_pi\n",
    "\n",
    "class QFunction(tf.keras.Model):\n",
    "    def __init__(self,odim,adim,hdims=[256,256],actv='relu'):\n",
    "        super().__init__()\n",
    "        self.q = mlp(odim+adim, hdims=hdims+[1], actv=actv, output_actv=None)\n",
    "    @tf.function\n",
    "    def call(self, o, a):\n",
    "        x = tf.concat([o, a], -1)\n",
    "        q = self.q(x)\n",
    "        return tf.squeeze(q, axis=1)\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self,odim,adim,hdims=[256,256],actv='relu',\n",
    "                 alpha_pi=0.1,alpha_q=0.1,gamma=0.98,lr=3e-4,epsilon=1e-3):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        self.odim       = odim\n",
    "        self.adim       = adim\n",
    "        self.hdims      = hdims\n",
    "        self.actv       = actv\n",
    "        \n",
    "        self.alpha_pi   = alpha_pi\n",
    "        self.alpha_q    = alpha_q\n",
    "        self.gamma      = gamma\n",
    "        self.lr         = lr\n",
    "        self.epsilon    = epsilon\n",
    "        \n",
    "        # Define policy and value functions\n",
    "        self.policy = GaussianPolicy(\n",
    "            odim=self.odim,adim=self.adim,hdims=self.hdims,actv=self.actv)\n",
    "        self.q1 = QFunction(\n",
    "            odim=self.odim,adim=self.adim,hdims=self.hdims,actv=self.actv)\n",
    "        self.q2 = QFunction(\n",
    "            odim=self.odim,adim=self.adim,hdims=self.hdims,actv=self.actv)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.train_pi = tf.keras.optimizers.Adam(learning_rate=self.lr,epsilon=self.epsilon)\n",
    "        self.train_q1 = tf.keras.optimizers.Adam(learning_rate=self.lr,epsilon=self.epsilon)\n",
    "        self.train_q2 = tf.keras.optimizers.Adam(learning_rate=self.lr,epsilon=self.epsilon)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, o, deterministic=False):\n",
    "        mu, pi, _ = self.policy(o, False)\n",
    "        if deterministic: return mu\n",
    "        else: return pi\n",
    "\n",
    "    @tf.function\n",
    "    def update_policy(self, data):\n",
    "        o = data['obs1']\n",
    "        with tf.GradientTape() as tape:\n",
    "            _, pi, logp_pi = self.policy(o)\n",
    "            q1_pi = self.q1(o, pi)\n",
    "            q2_pi = self.q2(o, pi)\n",
    "            min_q_pi = tf.minimum(q1_pi, q2_pi)\n",
    "            pi_loss = tf.reduce_mean(self.alpha_pi*logp_pi - min_q_pi)\n",
    "        variables = self.policy.trainable_variables\n",
    "        self.train_pi.minimize(pi_loss, variables, tape=tape)\n",
    "        return pi_loss, logp_pi, min_q_pi\n",
    "\n",
    "    @tf.function\n",
    "    def update_Q(self, target, data):\n",
    "        o,a,r,o2,d = data['obs1'],data['acts'],data['rews'],data['obs2'],data['done']\n",
    "        _, pi_next, logp_pi_next = self.policy(o2)\n",
    "        q1_targ = target.q1(o2, pi_next)\n",
    "        q2_targ = target.q2(o2, pi_next)\n",
    "        min_q_targ = tf.minimum(q1_targ, q2_targ)\n",
    "        q_backup = tf.stop_gradient(\n",
    "            r + self.gamma*(1-d)*(min_q_targ - self.alpha_q*logp_pi_next)\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            q1 = self.q1(o, a)\n",
    "            q2 = self.q2(o, a)\n",
    "            q1_loss = 0.5*tf.losses.mse(q1,q_backup)\n",
    "            q2_loss = 0.5*tf.losses.mse(q2,q_backup)\n",
    "            value_loss = q1_loss + q2_loss\n",
    "        self.train_q1.minimize(\n",
    "            value_loss,self.q1.trainable_variables+self.q2.trainable_variables,tape=tape)\n",
    "        return value_loss, q1, q2, logp_pi_next, q_backup, q1_targ, q2_targ\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-training",
   "metadata": {},
   "source": [
    "### SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_envs():\n",
    "    env_name = 'AntBulletEnv-v0'\n",
    "    env,eval_env = gym.make(env_name),gym.make(env_name)\n",
    "    _ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering\n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return env,eval_env\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self,hdims=[256,256],alpha_pi=0.1,alpha_q=0.1,gamma=0.98,polyak=0.995,\n",
    "                 lr=3e-4,epsilon=1e-3,seed=1,\n",
    "                 buffer_size_short=1e5,buffer_size_long=1e6):\n",
    "        \"\"\"\n",
    "        Initialize SAC agent\n",
    "        \"\"\"\n",
    "        self.hdims              = hdims\n",
    "        self.alpha_pi           = alpha_pi\n",
    "        self.alpha_q            = alpha_q\n",
    "        self.gamma              = gamma\n",
    "        self.polyak             = polyak\n",
    "        \n",
    "        self.lr                 = lr\n",
    "        self.epsilon            = epsilon\n",
    "        self.seed               = seed\n",
    "        \n",
    "        self.buffer_size_short  = buffer_size_short\n",
    "        self.buffer_size_long   = buffer_size_long\n",
    "        \n",
    "        # Environment\n",
    "        self.env, self.eval_env = get_envs()\n",
    "        odim, adim    = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim     = odim\n",
    "        self.adim     = adim\n",
    "\n",
    "        # Actor-critic model\n",
    "        tf.random.set_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        self.model = ActorCritic(self.odim,self.adim,hdims=self.hdims,\n",
    "                                 alpha_pi=self.alpha_pi,alpha_q=self.alpha_q,gamma=self.gamma,\n",
    "                                 lr=self.lr,epsilon=self.epsilon)\n",
    "        self.target = ActorCritic(self.odim,self.adim,hdims=self.hdims,\n",
    "                                  alpha_pi=self.alpha_pi,alpha_q=self.alpha_q,gamma=self.gamma,\n",
    "                                  lr=self.lr,epsilon=1e-3)\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "        # Buffers\n",
    "        self.replay_buffer_long = SACBuffer(odim=self.odim,adim=self.adim,\n",
    "                                            size=int(self.buffer_size_long))\n",
    "        self.replay_buffer_short = SACBuffer(odim=self.odim,adim=self.adim,\n",
    "                                             size=int(self.buffer_size_short))\n",
    "\n",
    "    def get_action(self, o, deterministic=False):\n",
    "        return self.model(tf.constant(o.reshape(1,-1)),deterministic)\n",
    "\n",
    "    def get_weights(self):\n",
    "        weight_vals = self.model.state_dict()\n",
    "        return weight_vals\n",
    "\n",
    "    def set_weights(self, weight_vals):\n",
    "        return self.model.load_state_dict(weight_vals)\n",
    "\n",
    "    @tf.function\n",
    "    def update_sac(self, replay_buffer):\n",
    "        pi_loss, logp_pi, min_q_pi = self.model.update_policy(replay_buffer)\n",
    "        value_loss, q1, q2, logp_pi_next, q_backup, q1_targ, q2_targ = \\\n",
    "            self.model.update_Q(self.target, replay_buffer)\n",
    "\n",
    "        # Polyak averaging of value networks\n",
    "        for v_main, v_targ in zip(self.model.q1.trainable_variables,\n",
    "                                  self.target.q1.trainable_variables):\n",
    "            v_targ.assign(v_main * (1-self.polyak) + v_targ * self.polyak)\n",
    "        for v_main, v_targ in zip(self.model.q2.trainable_variables,\n",
    "                                  self.target.q2.trainable_variables):\n",
    "            v_targ.assign(v_main * (1-self.polyak) + v_targ * self.polyak)\n",
    "\n",
    "        return logp_pi, min_q_pi, logp_pi_next, q_backup, q1_targ, q2_targ\n",
    "\n",
    "    def train(self,total_steps=1e6,start_steps=1e4,evaluate_every=1e4,plot_every=1e4,\n",
    "              batch_size=128,update_count=2,max_ep_len_eval=1000,load_dir=None):\n",
    "        \"\"\"\n",
    "        Train SAC\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        [v_targ.assign(v_main) for v_main, v_targ in zip(\n",
    "            self.model.trainable_variables, self.target.trainable_variables\n",
    "        )]\n",
    "        o, r, d, ep_ret, ep_len, n_env_step = self.env.reset(), 0, False, 0, 0, 0\n",
    "        for step in range(int(total_steps)):\n",
    "            # Step\n",
    "            if step > start_steps:\n",
    "                a = self.get_action(o, deterministic=False)\n",
    "                a = a.numpy()[0]\n",
    "            else:\n",
    "                a = self.env.action_space.sample()\n",
    "            o2, r, d, _ = self.env.step(a)\n",
    "            ep_len += 1\n",
    "            ep_ret += r\n",
    "\n",
    "            # Append\n",
    "            self.replay_buffer_long.store(o, a, r, o2, d)\n",
    "            self.replay_buffer_short.store(o, a, r, o2, d)\n",
    "            n_env_step += 1\n",
    "            o = o2\n",
    "\n",
    "            # Reset when done\n",
    "            if d: o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            # Update\n",
    "            if step >= start_steps:\n",
    "                for _ in tf.range(update_count):\n",
    "                    batch = self.replay_buffer_long.sample_batch(batch_size//2)\n",
    "                    batch_short = self.replay_buffer_short.sample_batch(batch_size//2)\n",
    "                    batch = {k: tf.constant(v) for k, v in batch.items()}\n",
    "                    batch_short = {k: tf.constant(v) for k, v in batch_short.items()}\n",
    "                    replay_buffer = dict(obs1=tf.concat([batch['obs1'], batch_short['obs1']], 0),\n",
    "                                            obs2=tf.concat([batch['obs2'], batch_short['obs2']], 0),\n",
    "                                            acts=tf.concat([batch['acts'], batch_short['acts']], 0),\n",
    "                                            rews=tf.concat([batch['rews'], batch_short['rews']], 0),\n",
    "                                            done=tf.concat([batch['done'], batch_short['done']], 0))\n",
    "                    logp_pi, min_q_pi, logp_pi_next, q_backup, q1_targ, q2_targ = self.update_sac(replay_buffer)\n",
    "\n",
    "            # Evaluate\n",
    "            if (step==0) or (((step+1)%evaluate_every)==0) or (((step+1)%plot_every)==0):\n",
    "                ram_percent = psutil.virtual_memory().percent  # memory usage\n",
    "                print(\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\" %\n",
    "                      (step + 1, total_steps, step / total_steps * 100,\n",
    "                       n_env_step,\n",
    "                       time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)),\n",
    "                       ram_percent)\n",
    "                      )\n",
    "                o, d, ep_ret, ep_len = self.eval_env.reset(), False, 0, 0\n",
    "                _ = self.eval_env.render(mode='human')\n",
    "                frames = []\n",
    "                while not (d or (ep_len == max_ep_len_eval)):\n",
    "                    a = self.get_action(o, deterministic=True)\n",
    "                    o, r, d, _ = self.eval_env.step(a.numpy()[0])\n",
    "                    frame = self.eval_env.render(mode='rgb_array')\n",
    "                    texted_frame = cv2.putText(\n",
    "                        img=np.copy(frame),\n",
    "                        text='tick:[%d]'%(ep_len),\n",
    "                        org=(80,30),fontFace=2,fontScale=0.8,color=(0,0,255),thickness=1)\n",
    "                    if (ep_len%5) == 0:\n",
    "                        frames.append(texted_frame)\n",
    "                    ep_ret += r  # compute return\n",
    "                    ep_len += 1\n",
    "                if (step==0) or (((step+1)%plot_every)==0):\n",
    "                    display_frames_as_gif(frames)\n",
    "                print(\"[Eval. done] ep_ret:[%.4f] ep_len:[%d]\"% (ep_ret, ep_len))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-increase",
   "metadata": {},
   "source": [
    "### Train an Ant agent with SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-excellence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = Agent(hdims=[256,256],alpha_pi=0.1,alpha_q=0.1,gamma=0.98,polyak=0.995,\n",
    "          lr=1e-3,epsilon=1e-8,seed=1,buffer_size_short=5e3,buffer_size_long=1e5)\n",
    "A.train(total_steps=2e5,start_steps=1e4,evaluate_every=1e4,plot_every=5e4,\n",
    "        batch_size=128,update_count=2,max_ep_len_eval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "burning-medline",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
